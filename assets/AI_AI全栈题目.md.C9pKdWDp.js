import{_ as s,a,o as t,aH as l}from"./chunks/framework.2-_qtiUw.js";const n="/FE-Knowledge2/assets/WX20251015-170820@2x.BIVTjL5U.png",e="/FE-Knowledge2/assets/WX20251015-171559@2x.Z-gT-rFs.png",c=JSON.parse('{"title":"AI全栈题目","description":"","frontmatter":{},"headers":[],"relativePath":"AI/AI全栈题目.md","filePath":"AI/AI全栈题目.md","lastUpdated":1760588581000}'),o={name:"AI/AI全栈题目.md"};function p(h,i,r,k,d,g){return t(),a("div",null,[...i[0]||(i[0]=[l('<h1 id="ai全栈题目" tabindex="-1">AI全栈题目 <a class="header-anchor" href="#ai全栈题目" aria-label="Permalink to &quot;AI全栈题目&quot;">​</a></h1><h2 id="问题1-llm-是什么​" tabindex="-1">问题1：LLM 是什么​ <a class="header-anchor" href="#问题1-llm-是什么​" aria-label="Permalink to &quot;问题1：LLM 是什么​&quot;">​</a></h2><p>LLM 是 “Large Language Model”，中文叫 大语言模型。一种基于深度学习的人工智能模型，通过学习海量文本数据，掌握语言规律，从而能够理解、生成、推理和总结自然语言。从语言中学出逻辑， 而不仅仅是会说话。参数是模型“语言理解和知识记忆”的核心。参数越多，几十亿到上千亿个参数，模型的理解和生成能力越强。</p><h2 id="问题2-目前热门-llm-大模型​" tabindex="-1">问题2：目前热门 LLM 大模型​ <a class="header-anchor" href="#问题2-目前热门-llm-大模型​" aria-label="Permalink to &quot;问题2：目前热门 LLM 大模型​&quot;">​</a></h2><ul><li><p><strong>OpenAI 系列</strong>：GPT-4.1、GPT-5 是通用能力最强的代表，支持多模态、工具调用和长上下文，是企业级 AI 的首选。</p></li><li><p><strong>Claude 系列（Anthropic）</strong>：以安全、逻辑一致和长文本能力著称，在写作、知识推理方面表现出色。</p></li><li><p><strong>Gemini 系列（Google）</strong>：主打多模态，结合搜索和视频理解能力。</p></li><li><p><strong>LLaMA 系列（Meta）</strong>：最受欢迎的开源模型，广泛用于微调与本地部署，性能与开放性兼顾。</p></li><li><p><strong>DeepSeek 系列</strong>：强化推理与逻辑一致性，代表开源社区在数学和代码领域的突破。</p></li><li><p><strong>Qwen 系列（阿里）</strong>：中文表现最强的开源模型之一，适合国内业务与多轮对话场景。</p></li><li><p><strong>Kimi 系列（Moonshot）</strong>：主打超长上下文和文档理解，适合知识管理和阅读类应用。</p></li><li><p><strong>Mistral / Phi / Gemma</strong>：轻量高效，适合边缘部署与中小规模应用。</p></li></ul><h2 id="问题3-关于-ai-的名词解释​" tabindex="-1">问题3：关于 AI 的名词解释​ <a class="header-anchor" href="#问题3-关于-ai-的名词解释​" aria-label="Permalink to &quot;问题3：关于 AI 的名词解释​&quot;">​</a></h2><ul><li><p>Prompt（提示词）<br> 给大模型的输入指令，用来控制模型行为、回答风格或思维方式。<br> Prompt 工程就是通过设计语言，让模型 &quot;按你的逻辑思考&quot;。</p></li><li><p>LLM（Large Language Model）<br> 大语言模型，通过学习海量文本掌握语言规律，能理解、生成和推理自然语言。<br> 如： GPT、Claude、Qwen、Kimi、Gemini 等。</p></li><li><p>RAG（Retrieval-Augmented Generation）<br> 检索增强生成。先从向量数据库检索相关资料，再交给模型生成答案，<br> 可让模型&quot;有依据可依&quot;，减少幻觉。</p></li><li><p>Fine-tuning（微调）<br> 对已有模型进行二次训练，让它在特定领域表现更好。适合定制行业模型或个性化助手。</p></li><li><p>Agent（智能体）<br> 具备自主思考与行动能力的 AI 系统。能自己规划任务、调用工具、执行操作，是&quot;AI 工程师&quot;的雏形。</p></li><li><p>Tool（工具函数）<br> 模型在 Agent 模式中可调用的外部功能，比如查天气、调用接口、读数据库。</p></li><li><p>Function Calling（函数调用）<br> 模型输出 JSON，告诉你要调用哪个函数、传什么参数。是让模型&quot;做事&quot;的关键机制。</p></li><li><p>MCP（Model Context Protocol）<br> OpenAI 推出的模型交互协议，让模型与 IDE、浏览器、文件系统直接通信，是下一代 AI 操作系统标准。</p></li><li><p>Prompt Injection（提示注入）<br> 攻击方式：用户在输入中嵌入恶意指令，让模型执行错误行为。工程上需过滤和权限控制。</p></li><li><p>Embedding（向量嵌入）<br> 把文本转成高维向量表示语义。在搜索、RAG、语义匹配中常用。</p></li></ul><h2 id="问题4-关于全栈名词解释​" tabindex="-1">问题4：关于全栈名词解释​ <a class="header-anchor" href="#问题4-关于全栈名词解释​" aria-label="Permalink to &quot;问题4：关于全栈名词解释​&quot;">​</a></h2><ul><li><p><strong>BFF（Backend For Frontend）</strong><br> 前后端分离架构中，为前端量身定制的后端层，负责接口聚合、权限控制、Session 管理等。</p></li><li><p><strong>SSR（Server-Side Rendering）</strong><br> 服务端渲染。页面由服务器生成 HTML 后再返回浏览器，提高首屏性能和 SEO 效果。</p></li><li><p><strong>Monorepo（单仓多包）</strong><br> 一种代码管理方式，把多个子项目统一放在一个仓库中，用 pnpm、TurboRepo 等工具统一构建与依赖。</p></li><li><p><strong>CI/CD（持续集成 / 持续部署）</strong><br> 自动化构建与发布流程：代码提交后自动测试、打包、部署。如： Jenkins、GitHub Actions、GitLab CI 等</p></li><li><p><strong>Docker / 容器化</strong><br> 把应用及其依赖封装进独立环境中运行。可实现&quot;一次构建，到处运行&quot;。</p></li></ul><h2 id="问题5-大模型在-tob-领域中应用的常见问题​" tabindex="-1">问题5：大模型在 ToB 领域中应用的常见问题​ <a class="header-anchor" href="#问题5-大模型在-tob-领域中应用的常见问题​" aria-label="Permalink to &quot;问题5：大模型在 ToB 领域中应用的常见问题​&quot;">​</a></h2><ul><li>LLM 泛知识，无法回答企业内部知识，譬如员工电话号码，部门规章制度，相关守则 等等。​</li><li>LLM 会有致幻问题，有产生幻觉的可能，不适用于企业应用的正式环境。​ <strong>解决方案</strong>：为模型外挂一个知识库，辅助 LLM 回答问题。 （RAG）</li></ul><p><img src="'+n+'" alt=""></p><h2 id="问题6-llm-出现幻觉-hallucination-的深层原因是什么​" tabindex="-1">问题6：LLM 出现幻觉（Hallucination）的深层原因是什么​ <a class="header-anchor" href="#问题6-llm-出现幻觉-hallucination-的深层原因是什么​" aria-label="Permalink to &quot;问题6：LLM 出现幻觉（Hallucination）的深层原因是什么​&quot;">​</a></h2><ul><li><p><strong>语言模型是概率模型，不是事实模型</strong>：LLM 的本质是&quot;预测下一个最可能的词&quot;，不是在&quot;查找真相&quot;，而是在生成语言模式。当输入提示不明确或知识缺失时，会凭统计相关性&quot;合理地编造&quot;。</p></li><li><p><strong>训练数据中存在噪声和虚假样本</strong>：大模型学习了互联网上的海量文本，而这些内容本身可能包含错误或臆测信息。模型学到这些偏差后，在回答中会自然复现。</p></li><li><p><strong>缺乏事实验证机制</strong>：模型输出结果时不会自动校验真伪，也不会访问实时数据。在多轮推理中，错误会被&quot;递进强化&quot;——尤其是 Agent 模式下的反射循环，会放大错误逻辑。</p></li><li><p><strong>Prompt 上下文过短或缺乏约束</strong>：当上下文被截断、知识片段不完整，模型会自动&quot;补空缺&quot;，生成符合语义但不符合事实的回答。</p></li><li><p><strong>任务模糊或目标歧义</strong>：如果任务没有明确评价标准，模型会更倾向于填补内容空白，从而编造细节。</p></li></ul><h2 id="问题7-rag-检索增强生成-是什么-​" tabindex="-1">问题7 ：RAG （检索增强生成）是什么？​ <a class="header-anchor" href="#问题7-rag-检索增强生成-是什么-​" aria-label="Permalink to &quot;问题7 ：RAG （检索增强生成）是什么？​&quot;">​</a></h2><p>RAG（Retrieval-Augmented Generation）是当前企业级 AI 应用最核心的架构思路之一。​</p><p>让模型“具备最新知识”，而不依赖模型固有训练语料。​</p><ol><li><p><strong>文档嵌入（Embedding）</strong></p><ul><li>把知识库（PDF、Markdown、数据库内容等）切成小块（Chunk）</li><li>然后用 Embedding Model（如 text-embedding-3-large 或 bge-m3）将文本转为高维向量</li></ul></li><li><p><strong>向量检索（Vector Search）</strong></p><ul><li>用户提问时，将 Query 也转成向量</li><li>计算 Query 向量与文档向量的相似度</li><li>检索出最相关的若干段落</li></ul></li><li><p><strong>增强生成（Augmented Generation）</strong></p><ul><li>把检索结果拼入 Prompt 的上下文中</li><li>交由 LLM 生成最终回答</li></ul></li></ol><h2 id="问题8-rag-检索增强生成-的原理与工程实现方式。​" tabindex="-1">问题8：RAG（检索增强生成）的原理与工程实现方式。​ <a class="header-anchor" href="#问题8-rag-检索增强生成-的原理与工程实现方式。​" aria-label="Permalink to &quot;问题8：RAG（检索增强生成）的原理与工程实现方式。​&quot;">​</a></h2><ul><li><strong>录入流程</strong>：文档 -&gt; 切片 -&gt; 向量化处理 -&gt; 存入向量数据库​</li><li><strong>Agent 流程</strong>：提问 -&gt; 问题向量化 -&gt; 语义检索（到向量数据库） -&gt; 答案排序整理 -&gt; LLM处理 -&gt; 回答</li></ul><p><img src="'+e+`" alt=""></p><h2 id="问题9-前端实现-llm-的流式输出" tabindex="-1">问题9：前端实现 LLM 的流式输出 <a class="header-anchor" href="#问题9-前端实现-llm-的流式输出" aria-label="Permalink to &quot;问题9：前端实现 LLM 的流式输出&quot;">​</a></h2><p>使用 <strong>Server-Sent Events (SSE)</strong>：可参考本文档网络板块的 SSE 实现。​</p><p>这里介绍 OpenAI SDK ：​</p><ul><li>提供 <code>stream: true</code> 参数；​</li><li>每生成一段内容就通过 <code>data:</code> 推送到前端。</li></ul><div class="language-js vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">js</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// 后端​</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> response</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai.chat.completions.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">create</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  model: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt-4o-mini&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  stream: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  messages: [{ role: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, content: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;asdwasxdjqdwqjqdwjb&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> }],​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">});​</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> chunk</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> of</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> response) {​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  res.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">write</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">\`data: \${</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">chunk</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">choices</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">]?.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">delta</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">?.</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">content</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> ||</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;&#39;}</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">\`</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">);​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">​</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// 前端​</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> eventSource</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> EventSource</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;/api/chat&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">);​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">eventSource.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">onmessage</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">e</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {​</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">  appendToChat</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(e.data);​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">};</span></span></code></pre></div><h2 id="问题10-function-calling-是什么-​" tabindex="-1">问题10 ：Function Calling 是什么 ​ <a class="header-anchor" href="#问题10-function-calling-是什么-​" aria-label="Permalink to &quot;问题10 ：Function Calling 是什么 ​&quot;">​</a></h2><p>Function Calling 是 OpenAI 提出的早期结构化调用机制。​ <strong>核心</strong>：让模型可以“主动调用外部函数”，而不是只生成文本。</p><h2 id="问题11-function-calling-原理流程​" tabindex="-1">问题11：Function Calling 原理流程​ <a class="header-anchor" href="#问题11-function-calling-原理流程​" aria-label="Permalink to &quot;问题11：Function Calling 原理流程​&quot;">​</a></h2><ol><li><p>开发者向模型注册函数定义（名称、参数 schema）。</p><div class="language-js vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">js</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">functions</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [{​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    name: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;getWeather&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    parameters: {​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        type: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;object&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        properties: { city: { type: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;string&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> } },​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        required: [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;city&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    }​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}]</span></span></code></pre></div></li><li><p>模型推理后生成结构化 JSON（指明要调用哪个函数、参数是什么）。</p><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">​</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    &quot;name&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;getWeather&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">​</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    &quot;arguments&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: { </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">&quot;city&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Guangzhou&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> }</span><span style="--shiki-light:#B31D28;--shiki-light-font-style:italic;--shiki-dark:#FDAEB7;--shiki-dark-font-style:italic;">​</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div></li><li><p>开发者执行该函数，并把结果返回给模型，模型再继续生成答案。</p><div class="language-js vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">js</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> result</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> getWeather</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Guangzhou&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div></li></ol><h2 id="问题12-function-calling-优缺点​" tabindex="-1">问题12：Function Calling 优缺点​ <a class="header-anchor" href="#问题12-function-calling-优缺点​" aria-label="Permalink to &quot;问题12：Function Calling 优缺点​&quot;">​</a></h2><p>优点：​</p><ul><li>简单易用；​</li><li>提高模型可控性；​</li><li>支持多步对话任务执行。​</li></ul><p>缺点：​</p><ul><li>只能在模型调用时“注册函数”，不具备统一协议；​</li><li>不支持跨语言、跨模型的工具共享；​</li><li>无法标准化工具描述和安全权限控制；​</li><li>工具执行结果必须人工中转。</li></ul><h2 id="问题13-mcp-是什么-​" tabindex="-1">问题13：MCP 是什么？​ <a class="header-anchor" href="#问题13-mcp-是什么-​" aria-label="Permalink to &quot;问题13：MCP 是什么？​&quot;">​</a></h2><p>MCP = Model Context Protocol）​</p><p>它是 OpenAI 在 2024 年底正式推出的下一代 AI Agent 协议标准。​</p><p>它是 Function Calling 的“体系化升级版”。​</p><p>定义一套 模型 ↔ 工具 ↔ 上下文 的标准通信协议，让模型具备“插件化操作系统”能力。</p><h2 id="问题14-mcp-的核心结构​" tabindex="-1">问题14：MCP 的核心结构​ <a class="header-anchor" href="#问题14-mcp-的核心结构​" aria-label="Permalink to &quot;问题14：MCP 的核心结构​&quot;">​</a></h2><p>Server（工具服务端）：提供一组可用的工具（如文件读写、数据库、HTTP请求等）；​</p><p>Client（模型或 IDE）：通过协议访问 Server；​</p><p>Bridge（中间层）：负责转发、权限、上下文同步。​</p><p>交互流程：LLM -&gt; MCP Client -&gt; MCP Server（工具集）-&gt; 外部资源</p><h2 id="问题15-function-calling-与-mcp-实现差异" tabindex="-1">问题15：Function Calling 与 MCP 实现差异 <a class="header-anchor" href="#问题15-function-calling-与-mcp-实现差异" aria-label="Permalink to &quot;问题15：Function Calling 与 MCP 实现差异&quot;">​</a></h2><p><strong>Function Calling 版本​</strong></p><p>模型调用 getStockPrice(&#39;AAPL&#39;) 获取苹果股价：</p><pre><code>\`\`\`js
// 模型 → JSON → 你手动调用 API → 返回结果 → 再交给模型。​
functions: [{​
    name: &quot;getStockPrice&quot;,​
    parameters: { type: &quot;object&quot;, properties: { symbol: { type: &quot;string&quot; } } }​
}]
\`\`\`
</code></pre><p><strong>MCP 版本​</strong></p><p>模型通过 MCP 直接访问股票工具服务：</p><pre><code>\`\`\`js
// MCP Server 自动处理，模型无需开发者中转​
// 调用外部 API -&gt; 校验权限 -&gt; 结果通过协议返回给模型​
{​
    &quot;action&quot;: &quot;tools.call&quot;,​
    &quot;tool&quot;: &quot;stocks.getPrice&quot;,​
    &quot;args&quot;: { &quot;symbol&quot;: &quot;AAPL&quot; }​
}
\`\`\`
</code></pre><p>​</p><h2 id="问题16-function-calling-与-mcp-对比" tabindex="-1">问题16：Function Calling 与 MCP 对比 <a class="header-anchor" href="#问题16-function-calling-与-mcp-对比" aria-label="Permalink to &quot;问题16：Function Calling 与 MCP 对比&quot;">​</a></h2><table tabindex="0"><thead><tr><th>对比维度</th><th>Function Calling</th><th>MCP</th></tr></thead><tbody><tr><td>定义者</td><td>OpenAI(2023)</td><td>OpenAI(2024,正式标准)</td></tr><tr><td>核心目标</td><td>模型调用外部函数</td><td>模型与外部环境标准化交互</td></tr><tr><td>安全机制</td><td>开发者自管</td><td>协议内建权限系统</td></tr><tr><td>语言兼容性</td><td>局限于SDK</td><td>任意语言（JSON-RPC）</td></tr><tr><td>工具发现</td><td>静态注册</td><td>动态发现/热加载</td></tr><tr><td>应用场景</td><td>单模型任务执行</td><td>多Agent协作/IDE集成/系统级控制</td></tr></tbody></table><h2 id="问题17-agent-是什么​" tabindex="-1">问题17：Agent 是什么​ <a class="header-anchor" href="#问题17-agent-是什么​" aria-label="Permalink to &quot;问题17：Agent 是什么​&quot;">​</a></h2><p>Agent（智能体）在 AI 语境下，指的是一种具备自主决策能力的执行单元。不是简单的「调用模型」，而是围绕一个目标，能够感知环境、规划步骤、调用工具、执行行动并根据反馈调整策略的系统。​</p><p>Agent = <strong>模型能力（大语言模型）</strong> + <strong>记忆</strong> + <strong>工具调用</strong> + <strong>状态感知与反馈循环</strong> + <strong>行动策略</strong></p><h2 id="问题18-agent-loop-是什么什么-​" tabindex="-1">问题18：Agent Loop 是什么什么 ​ <a class="header-anchor" href="#问题18-agent-loop-是什么什么-​" aria-label="Permalink to &quot;问题18：Agent Loop 是什么什么 ​&quot;">​</a></h2><p>Agent Loop（循环反射机制） 是大模型在执行任务时不断进行 计划 → 执行 → 观察结果 → 再计划 的循环过程，不再是一次性输出答案，而是会根据每一步的执行结果决定下一步要做的事情。​</p><p><strong>基本流程</strong>：​</p><ul><li>用户提出一个任务，比如&quot;查库存，不够就补货&quot;。​</li><li>Agent 首先生成一个计划（我应该先查库存）。​</li><li>Agent 调用工具或 API 执行计划（比如请求库存接口）。​</li><li>得到执行结果后，模型会&quot;反思&quot;当前状态（库存低于阈值，需要继续执行下一步）。​</li><li>再次生成计划，比如调用下单接口。​</li><li>直到模型判断任务已经完成，才停止循环并给出最终回答。​</li></ul><p><strong>LLM = 决策器，Tool = 手执行的工具，Loop = 自己驱动自己继续做事</strong></p><h2 id="问题19-agent-loop-常见问题与风险​" tabindex="-1">问题19：Agent Loop 常见问题与风险​ <a class="header-anchor" href="#问题19-agent-loop-常见问题与风险​" aria-label="Permalink to &quot;问题19：Agent Loop 常见问题与风险​&quot;">​</a></h2><ul><li><strong>容易进入死循环</strong>：模型可能一直在&quot;再确认、再检查&quot;，不停止，如不断重复 &quot;确认库存&quot; 这种无意义操作。​</li><li><strong>Token 消耗极高</strong>：每一次反射都要重新发送上下文，调用多轮 API，很快消耗大量 Token，造成巨大成本问题。​</li><li><strong>错误行为会被放大</strong>：如果第一次 Observation 理解错误，模型可能继续沿着错误方向一错再错。​</li><li><strong>缺乏安全边界</strong>：如果工具没有做白名单和参数校验，模型可能不加限制地执行敏感操作。</li></ul><h2 id="问题20-什么是模型微调-fine-tuning-​" tabindex="-1">问题20：什么是模型微调（Fine-tuning）？​ <a class="header-anchor" href="#问题20-什么是模型微调-fine-tuning-​" aria-label="Permalink to &quot;问题20：什么是模型微调（Fine-tuning）？​&quot;">​</a></h2><p>Fine-tuning 是在预训练模型基础上，使用特定领域的数据再次训练，让模型在该领域表现更好。当于“教模型新知识”或“让模型形成固定风格”。重新训练部分或全部权重，使模型从数据中学习新模式。</p><h2 id="问题21-llm-微调中常见的两种类型是什么-各适合什么场景-​" tabindex="-1">问题21：LLM 微调中常见的两种类型是什么？各适合什么场景？​ <a class="header-anchor" href="#问题21-llm-微调中常见的两种类型是什么-各适合什么场景-​" aria-label="Permalink to &quot;问题21：LLM 微调中常见的两种类型是什么？各适合什么场景？​&quot;">​</a></h2><ul><li>全参数微调（Full Fine-tuning）​ <ul><li>调整模型全部参数。​</li><li>优点：效果最好。缺点：计算量大、成本高。​</li><li>适合：大型机构、科研或完全垂直场景（如法律、医学）。​</li></ul></li><li>轻量微调（Parameter Efficient Fine-tuning, PEFT）​ <ul><li>只调整少量参数，如 LoRA、QLoRA、Prefix-Tuning。​</li><li>优点：显著降低显存需求；缺点：泛化略弱。​</li><li>适合：中小团队做领域定制或角色微调。</li></ul></li></ul><h2 id="问题22-lora-是如何实现高效微调的-​" tabindex="-1">问题22：LoRA 是如何实现高效微调的？​ <a class="header-anchor" href="#问题22-lora-是如何实现高效微调的-​" aria-label="Permalink to &quot;问题22：LoRA 是如何实现高效微调的？​&quot;">​</a></h2><p>LoRA（Low-Rank Adaptation）在原模型权重矩阵旁边插入两个低秩矩阵（A、B），在训练时只更新这两个小矩阵，原始权重冻结。相当于 W&#39; = W + A × B这样显存消耗下降 90%+，训练速度提升数倍。​</p><p>LoRA 的核心思想是“用低维近似表达参数变化”。</p><h2 id="问题23-微调一个-llm-需要准备哪些数据-格式上有什么要求-​" tabindex="-1">问题23：微调一个 LLM 需要准备哪些数据？格式上有什么要求？​ <a class="header-anchor" href="#问题23-微调一个-llm-需要准备哪些数据-格式上有什么要求-​" aria-label="Permalink to &quot;问题23：微调一个 LLM 需要准备哪些数据？格式上有什么要求？​&quot;">​</a></h2><ul><li>指令数据（Instruction + Response）：模型学会根据任务指令输出结果。​</li><li>对话数据（Chat Format）：多轮上下文格式，适合聊天类模型。​</li><li>知识数据（Context + QA）：强化领域知识问答能力。​</li></ul><p><strong>注意事项</strong>：​</p><ul><li>样本数量不求多，但要高质量、结构一致；​</li><li>去除噪声与矛盾样本；​</li><li>统一 token 长度与风格。</li></ul><h2 id="问题24-dify-是什么-​" tabindex="-1">问题24：Dify 是什么？​ <a class="header-anchor" href="#问题24-dify-是什么-​" aria-label="Permalink to &quot;问题24：Dify 是什么？​&quot;">​</a></h2><p>Dify 是一个开源的 AI 应用构建平台（AI App Builder），可以让开发者和非技术人员通过可视化界面快速搭建大模型应用。把 Prompt、变量、知识库、工作流、模型调用都组件化，无需从零写代码即可实现一个可用的 ChatBot、问答系统或业务助手。本质上，Dify 是连接“模型能力”和“业务场景”的中间层。</p><h2 id="问题25-dify-的-workflow-工作流-是什么-​" tabindex="-1">问题25：Dify 的 Workflow（工作流）是什么？​ <a class="header-anchor" href="#问题25-dify-的-workflow-工作流-是什么-​" aria-label="Permalink to &quot;问题25：Dify 的 Workflow（工作流）是什么？​&quot;">​</a></h2><p>Workflow 是 Dify 的核心功能之一，用于定义 AI 应用的执行流程。解决了单一 Prompt 无法处理复杂逻辑的问题， 让模型具备“多步决策”和“自动化任务执行”的能力。可以让模型在多个步骤中依次调用：​</p><ul><li>模型推理（LLM Node）​</li><li>外部 API（HTTP Node）​</li><li>条件判断（If/Else Node）​</li><li>工具执行（Tool Node）​</li><li>数据处理（Code Node）</li></ul><h2 id="问题26-dify-如何接入外部-api-或数据库-​" tabindex="-1">问题26：Dify 如何接入外部 API 或数据库？​ <a class="header-anchor" href="#问题26-dify-如何接入外部-api-或数据库-​" aria-label="Permalink to &quot;问题26：Dify 如何接入外部 API 或数据库？​&quot;">​</a></h2><ul><li>HTTP 节点：在 Workflow 中可直接调用第三方 API；​</li><li>插件（Tool）机制：可自定义工具包，通过 REST 接口或函数实现；​</li><li>Webhook 回调：可让外部系统触发 Dify 应用或获取执行结果；​</li><li>SDK / API 调用：开发者可通过 Dify SDK 在代码中与应用通信。</li></ul><h2 id="问题27-dify-的知识库-knowledge-base-在-rag-中起什么作用-​" tabindex="-1">问题27：Dify 的知识库（Knowledge Base）在 RAG 中起什么作用？​ <a class="header-anchor" href="#问题27-dify-的知识库-knowledge-base-在-rag-中起什么作用-​" aria-label="Permalink to &quot;问题27：Dify 的知识库（Knowledge Base）在 RAG 中起什么作用？​&quot;">​</a></h2><p>Dify 的知识库是实现 RAG（检索增强生成）的关键模块。当用户上传文档、网页或文本时，Dify 会自动：​ 分片（Chunking）：将文档拆成小块；​</p><ol><li>向量化（Embedding）：把文本转为向量；​</li><li>语义检索（Retrieval）：根据用户问题查找最相关内容；​</li><li>上下文拼接（Augmentation）：把检索结果加入 Prompt；​</li><li>生成回答（Generation）：LLM 生成基于事实的回答。</li></ol><h2 id="问题28-在-dify-中-如何让一个应用支持多轮上下文对话-​" tabindex="-1">问题28：在 Dify 中，如何让一个应用支持多轮上下文对话？​ <a class="header-anchor" href="#问题28-在-dify-中-如何让一个应用支持多轮上下文对话-​" aria-label="Permalink to &quot;问题28：在 Dify 中，如何让一个应用支持多轮上下文对话？​&quot;">​</a></h2><p>Dify 内部有 Session 管理机制，每次用户对话会自动维护上下文变量（Messages）。开发者可通过：​</p><ul><li>设置「对话记忆」开关；​</li><li>控制上下文保留的轮数；​</li><li>在 Prompt 中引用历史内容 {conversation_history}；​</li></ul><p>来实现连续对话、上下文理解和逻辑衔接。这样用户可以和 AI 连续交流，不会“忘记前一句话”。</p><h2 id="问题29-dify-中-智能体-和-工作流-的关系​" tabindex="-1">问题29：Dify 中 智能体 和 工作流 的关系​ <a class="header-anchor" href="#问题29-dify-中-智能体-和-工作流-的关系​" aria-label="Permalink to &quot;问题29：Dify 中 智能体 和 工作流 的关系​&quot;">​</a></h2><p>智能体是执行任务的主体，而工作流是智能体执行任务的流程逻辑。智能体是谁来做，做什么，工作流是怎么做。智能体是AI的大脑，工作流是AI的行动路线。两者结合让 AI 真正落地成可控的业务流程。​</p><p><strong>智能体（Agent）​</strong></p><p>是具备自主决策与调用能力的 AI 实体。它基于系统提示（System Prompt）、工具（Tools）和记忆机制来完成任务。你可以在 Dify 里配置一个 Agent，让它具备特定的知识库、角色和功能，比如“财务助理”或“产品顾问”。​</p><p><strong>工作流（Workflow）​</strong></p><p>是一条任务执行链，用可视化节点描述任务的执行顺序。每个节点可以是一个模型调用、判断逻辑、API 请求、工具操作、甚至另一个 Agent 调用。工作流让任务执行可控、可调试、可复用。​</p><p><strong>工作流是智能体的执行骨架​</strong></p><p>智能体可以看作一个“驱动核心”，但如果要让它完成多步骤逻辑，就需要工作流定义步骤。​</p><ul><li>智能体：理解用户意图；​</li><li>工作流：调用不同节点 → 检索知识库 → 调 API → 汇总输出。​</li></ul><p><strong>智能体可以被工作流调用​</strong></p><p>在工作流中，一个节点可以是「Agent Node」，用于调用特定智能体执行子任务。​ 比如一个“客服工作流”中可调用不同智能体：​</p><ul><li>FAQ 智能体 → 回答常见问题；​</li><li>工单智能体 → 记录或升级问题。​</li></ul><p><strong>两者互补​</strong></p><ul><li>工作流负责“逻辑控制、条件判断、执行顺序”；​</li><li>智能体负责“语言理解、推理、任务决策”。合起来就是“会思考 + 会执行”的完整自动化系统。</li></ul>`,104)])])}const E=s(o,[["render",p]]);export{c as __pageData,E as default};
